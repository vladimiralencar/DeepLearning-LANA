{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDA-Jupyter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning_LANA/blob/master/CUDA/CUDA_Jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uxb1Kq5YAwKZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Nvidia CUDA em GPUs - Revisão"
      ]
    },
    {
      "metadata": {
        "id": "rwsuiFpiAwKa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Status da GPU"
      ]
    },
    {
      "metadata": {
        "id": "gFx8c-fYAwKb",
        "colab_type": "code",
        "outputId": "7d51383f-ee3d-4898-88c9-ac72b05c7fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jan 14 15:41:09 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hNvWrpipAzJL",
        "colab_type": "code",
        "outputId": "76974e7a-b019-44e6-ef81-0bf120b3b0d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 01-check-env.py\t\t  exemplo2.cu\n",
            " 02-PyCudaWorkflow.py\t\t  exemplo3.cu\n",
            " 03-PyCudaMatrixManipulation.py   exemplo4.cu\n",
            " 04-PyCudaGPUArray.py\t\t  exemplo5.cu\n",
            " 05-PyCudaElementWise.py\t  exemplo6.cu\n",
            " 06-PyCudaReductionKernel.py\t  exemplo7.cu\n",
            " cmemory\t\t\t  memory\n",
            " cmemory.cu\t\t\t  memory.cu\n",
            "'CUDA-Jupyter (1).ipynb'\t  opatomicas\n",
            " CUDA-Jupyter.ipynb\t\t 'opatomicas (1).cu'\n",
            " dotproduct\t\t\t 'opatomicas (2).cu'\n",
            "'dotproduct (1).cu'\t\t 'opatomicas (3).cu'\n",
            " dotproduct2\t\t\t  opatomicas.cu\n",
            "'dotproduct (2).cu'\t\t  pinnedmemory\n",
            "'dotproduct (3).cu'\t\t  pinnedmemory.cu\n",
            "'dotproduct (4).cu'\t\t  sample_data\n",
            " dotproduct.cu\t\t\t  smemory\n",
            " Duvida-Pycuda-01.txt\t\t  smemory.cu\n",
            " eventos\t\t\t  t2est-02-PyCudaWorkflow-test.py\n",
            " eventos.cu\t\t\t  test-02-PyCudaWorkflow-test.py\n",
            " exemplo1.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nkRzZB8kBFBU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Para importar arquivos"
      ]
    },
    {
      "metadata": {
        "id": "NnVnEKqjBESe",
        "colab_type": "code",
        "outputId": "6dd02002-b4da-4f0e-8304-0f3e535bafa0",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5f5090e6-9cd4-4a98-ac59-638c25b09a20\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5f5090e6-9cd4-4a98-ac59-638c25b09a20\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving exemplo7.cu to exemplo7 (1).cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JXtD1ck1AwKh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Paralelismo\n",
        "\n",
        "Para a primeira tarefa, vamos usar os seguintes conceitos:\n",
        "\n",
        "* <code style=\"color:green\">&#95;&#95;global&#95;&#95;</code> - Esta palavra-chave é um qualificador usado para dizer ao compilador CUDA que a função deve ser compilada para a GPU. Para o CUDA C/C ++, o compilador nvcc irá lidar com a compilação deste código.\n",
        "* <code style=\"color:green\">blockIdx.x</code> - Esta é uma variável usada dentro de um kernel de GPU para determinar a ID do bloco que está atualmente executando o código. Uma vez que haverá muitos blocos em paralelo, precisamos desta ID para ajudar a determinar qual parte dos dados um bloco particular funcionará.\n",
        "* <code style=\"color:green\">threadIdx.x</code> - Esta é uma variável usada dentro de um kernel de GPU para determinar o ID da thread que está atualmente executando o código no bloco ativo.\n",
        "* <code style=\"color:green\">blockDim.x</code> - Esta é uma variável que retorna um valor que indica o número de threads que há por bloco. Lembre-se de que todos os blocos agendados para executar na GPU são idênticos, exceto para o valor de <code style=\"color:green\">blockIdx.x</code>.\n",
        "* <code style=\"color:green\">myKernel <<< numero_de_blocos, threads_por_bloco>>> (...)</code> -  Esta é a sintaxe usada para iniciar um kernel na GPU. Dentro de \"<<< >>>\", estabelecemos dois valores. O primeiro é o número total de blocos que queremos executar na GPU, e o segundo é o número de threads que há por bloco. \n",
        "\n",
        "Vamos explorar os conceitos acima, fazendo um simples exemplo de \"Hello Paralelismo\". Ao executar a célula abaixo, teremos:\n",
        "\n",
        "1. A partir do arquivo de origem .cu, código separado que deve ser compilado para a GPU e o código que deve ser compilado para a CPU\n",
        "2. O nvcc compilará o próprio código GPU\n",
        "3. nvcc dará ao compilador do host, no nosso caso gcc, o código da CPU para compilar\n",
        "4. Vincula o código compilado de # 2 e # 3 e crie o executável"
      ]
    },
    {
      "metadata": {
        "id": "9MiJ9z5bAwKi",
        "colab_type": "code",
        "outputId": "0f0c319f-8c4e-44f1-ece9-346b78a59921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "!cat exemplo1.cu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "#define NUM_BLOCKS 16\n",
            "#define BLOCK_WIDTH 1\n",
            "\n",
            "__global__ void hello()\n",
            "{\n",
            "    printf(\"Olá! Eu sou uma thread no bloco %d\\n\", blockIdx.x);\n",
            "}\n",
            "\n",
            "\n",
            "int main(int argc,char **argv)\n",
            "{\n",
            "    // Inicializa o kernel\n",
            "    hello<<<NUM_BLOCKS, BLOCK_WIDTH>>>();\n",
            "\n",
            "    // Sincroniza todas as threads antes de passar o controle de volta para a CPU\n",
            "    cudaDeviceSynchronize();\n",
            "\n",
            "    printf(\"Processamento Concluído!\\n\");\n",
            "\n",
            "    return 0;\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z0kod7UOAwKn",
        "colab_type": "code",
        "outputId": "6052ffb9-1e50-460a-e5ad-8ed57980a0e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "# Compila o exemplo1 e executa o programa gerado\n",
        "!nvcc -o exemplo1_out exemplo1.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Olá! Eu sou uma thread no bloco 5\n",
            "Olá! Eu sou uma thread no bloco 3\n",
            "Olá! Eu sou uma thread no bloco 12\n",
            "Olá! Eu sou uma thread no bloco 10\n",
            "Olá! Eu sou uma thread no bloco 13\n",
            "Olá! Eu sou uma thread no bloco 7\n",
            "Olá! Eu sou uma thread no bloco 6\n",
            "Olá! Eu sou uma thread no bloco 14\n",
            "Olá! Eu sou uma thread no bloco 8\n",
            "Olá! Eu sou uma thread no bloco 15\n",
            "Olá! Eu sou uma thread no bloco 1\n",
            "Olá! Eu sou uma thread no bloco 0\n",
            "Olá! Eu sou uma thread no bloco 11\n",
            "Olá! Eu sou uma thread no bloco 2\n",
            "Olá! Eu sou uma thread no bloco 9\n",
            "Olá! Eu sou uma thread no bloco 4\n",
            "Processamento Concluído!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7E8zJKfuAwKs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inicializando um kernel na GPU - Unified Memory"
      ]
    },
    {
      "metadata": {
        "id": "3MXA87cFAwKt",
        "colab_type": "code",
        "outputId": "cfb4c054-ec3a-4fcc-cda6-11cc72755da2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1105
        }
      },
      "cell_type": "code",
      "source": [
        "!cat exemplo2.cu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#include <stdio.h>\n",
            "#include <iostream>\n",
            "\n",
            "// Número de elementos em cada vetor\n",
            "#define N 2048 * 2048 \n",
            "\n",
            "__global__ void my_kernel(int * a, int * b, int * c)\n",
            "{\n",
            "    // Determina a identificação de thread global exclusiva, por isso sabemos qual elemento processar\n",
            "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
            "    \n",
            "    if ( tid < N ) // Certifique-se de que não inicializamos mais threads do que o necessário\n",
            "        c[tid] = a[tid] + b[tid];\n",
            "}\n",
            "\n",
            "void report_gpu_mem()\n",
            "{\n",
            "    size_t free, total;\n",
            "    cudaMemGetInfo(&free, &total);\n",
            "    std::cout << \"Free = \" << free << \" Total = \" << total <<std::endl;\n",
            "}\n",
            "\n",
            "int main()\n",
            "{\n",
            "    int *a, *b, *c;\n",
            "\n",
            "    // Número total de bytes por vetor\n",
            "    int size = N * sizeof (int); \n",
            "\n",
            "    // Aloca memória sem a necessidade de usar cudaMemcpy\n",
            "    cudaMallocManaged(&a, size);\n",
            "    cudaMallocManaged(&b, size);\n",
            "    cudaMallocManaged(&c, size);\n",
            "\n",
            "    // Inicializa memória\n",
            "    for( int i = 0; i < N; ++i )\n",
            "    {\n",
            "        a[i] = i;\n",
            "        b[i] = i;\n",
            "        c[i] = 0;\n",
            "    }\n",
            "\n",
            "    int threads_per_block = 128;\n",
            "    int number_of_blocks = (N / threads_per_block) + 1;\n",
            "\n",
            "    my_kernel <<< number_of_blocks, threads_per_block >>> ( a, b, c );\n",
            "\n",
            "    // Espera até a GPU finalizar\n",
            "    cudaDeviceSynchronize(); \n",
            "\n",
            "    // Imprime os últimos 5 valores de c \n",
            "    for( int i = N-5; i < N; ++i )\n",
            "        printf(\"c[%d] = %d, \", i, c[i]);\n",
            "    printf (\"\\n\");\n",
            "\n",
            "    // Libera toda a nossa memória alocada\n",
            "    report_gpu_mem();\n",
            "    cudaFree( a );\n",
            "    report_gpu_mem(); \n",
            "    cudaFree( b );\n",
            "    report_gpu_mem(); \n",
            "    cudaFree( c );\n",
            "    report_gpu_mem();\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-rNjLDvCAwKx",
        "colab_type": "code",
        "outputId": "022bc7dc-04ef-419f-b3ed-62f9cba9a4bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# Compila o exemplo2 e executa o programa gerado\n",
        "!nvcc  -o exemplo2_out exemplo2.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c[4194299] = 8388598, c[4194300] = 8388600, c[4194301] = 8388602, c[4194302] = 8388604, c[4194303] = 8388606, \n",
            "Free = 11872174080 Total = 11996954624\n",
            "Free = 11888951296 Total = 11996954624\n",
            "Free = 11905728512 Total = 11996954624\n",
            "Free = 11922505728 Total = 11996954624\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tijEgRuCAwK1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Acelerando Operações com Matrizes"
      ]
    },
    {
      "metadata": {
        "id": "fx1HcUeqAwK2",
        "colab_type": "code",
        "outputId": "226d8d5c-b1b5-43bd-9418-707af18363fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2125
        }
      },
      "cell_type": "code",
      "source": [
        "!cat exemplo3.cu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#include <stdio.h>\n",
            "#include \"cuda_runtime.h\"\n",
            "#include \"device_launch_parameters.h\"\n",
            "#include <iostream>\n",
            "#include <time.h>\n",
            "using namespace std;\n",
            "\n",
            "#define N 756\n",
            "\n",
            "// kernel\n",
            "__global__ void matrixMulGPU( int * a, int * b, int * c )\n",
            "{\n",
            "    int val = 0;\n",
            "\n",
            "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
            "    int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
            "\n",
            "    if (row < N && col < N)\n",
            "    {\n",
            "        for ( int k = 0; k < N; ++k )\n",
            "            val += a[row * N + k] * b[k * N + col];\n",
            "        c[row * N + col] = val;\n",
            "    }\n",
            "}\n",
            "\n",
            "void matrixMulCPU( int * a, int * b, int * c )\n",
            "{\n",
            "    int val = 0;\n",
            "\n",
            "    for( int row = 0; row < N; ++row )\n",
            "        for( int col = 0; col < N; ++col )\n",
            "        {\n",
            "            val = 0;\n",
            "            for ( int k = 0; k < N; ++k )\n",
            "                val += a[row * N + k] * b[k * N + col];\n",
            "            c[row * N + col] = val;\n",
            "        }\n",
            "}\n",
            "\n",
            "int main()\n",
            "{\n",
            "    int *a, *b, *c_cpu, *c_gpu;\n",
            "\n",
            "    // Número de bytes de uma matriz N x N \n",
            "    int size = N * N * sizeof (int); \n",
            "\n",
            "    // Aloca memória\n",
            "    cudaMallocManaged (&a, size);\n",
            "    cudaMallocManaged (&b, size);\n",
            "    cudaMallocManaged (&c_cpu, size);\n",
            "    cudaMallocManaged (&c_gpu, size);\n",
            "\n",
            "    // Inicializa memória\n",
            "    for( int row = 0; row < N; ++row )\n",
            "        for( int col = 0; col < N; ++col )\n",
            "        {\n",
            "            a[row * N + col] = row;\n",
            "            b[row * N + col] = col+2;\n",
            "            c_cpu[row * N + col] = 0;\n",
            "            c_gpu[row * N + col] = 0;\n",
            "        }\n",
            "\n",
            "    // Bloco de threads 16 x 16     \n",
            "    dim3 threads_per_block (16, 16, 1); \n",
            "    dim3 number_of_blocks ((N / threads_per_block.x) + 1, (N / threads_per_block.y) + 1, 1);\n",
            "\n",
            "    // Define 2 eventos CUDA\n",
            "    cudaEvent_t start, end;\n",
            "\n",
            "    // Cria os eventos\n",
            "    cudaEventCreate(&start);\n",
            "    cudaEventCreate(&end);\n",
            "\n",
            "    // Registra o primeiro evento\n",
            "    cudaEventRecord(start);\n",
            "\n",
            "    // Chamada ao kernel\n",
            "    matrixMulGPU <<< number_of_blocks, threads_per_block >>> ( a, b, c_gpu );\n",
            "\n",
            "    // Registra o segundo evento\n",
            "    cudaEventRecord(end);\n",
            "\n",
            "    // Aguarda a GPU finalizar seu trabalho\n",
            "    cudaDeviceSynchronize(); \n",
            "\n",
            "    // Calcula o tempo usado no processamento\n",
            "    float elapsed;\n",
            "    cudaEventElapsedTime(&elapsed, start, end);\n",
            "\n",
            "    cout << \"Tempo de processamento na GPU igual a \" << elapsed << \" msec (aproximadamente 0.01108 segundos)\" << endl;\n",
            "\n",
            "    clock_t start1, end1;\n",
            "    double cpu_time_used;\n",
            "\n",
            "    start1 = clock();\n",
            "\n",
            "    // Chama a versão para CPU para checar nosso trabalho\n",
            "    matrixMulCPU( a, b, c_cpu );\n",
            "\n",
            "    // Calcula o tempo usado no processamento\n",
            "    end1 = clock();\n",
            "    cpu_time_used = ((double) (end1 - start1)) / CLOCKS_PER_SEC;\n",
            "\n",
            "    cout << \"Tempo de processamento na CPU igual a \" << cpu_time_used << \" sec\" << endl;\n",
            "\n",
            "    // Compara as duas respostas para garantir que elas sejam iguais\n",
            "    bool error = false;\n",
            "    for( int row = 0; row < N && !error; ++row )\n",
            "        for( int col = 0; col < N && !error; ++col )\n",
            "            if (c_cpu[row * N + col] != c_gpu[row * N + col])\n",
            "            {\n",
            "                printf(\"FOUND ERROR at c[%d][%d]\\n\", row, col);\n",
            "                error = true;\n",
            "                break;\n",
            "            }\n",
            "    if (!error)\n",
            "        printf(\"Successo! As duas matrizes são iguais, sendo executadas na CPU e na GPU!\\n\");\n",
            "\n",
            "    // Libera a memória\n",
            "    cudaFree(a); \n",
            "    cudaFree(b);\n",
            "    cudaFree( c_cpu ); \n",
            "    cudaFree( c_gpu );\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dsEQ_ILeAwK8",
        "colab_type": "code",
        "outputId": "76607753-7e98-4a76-e119-a44f8bebf011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Compila o exemplo3 e executa o programa gerado\n",
        "!nvcc  -o exemplo3_out exemplo3.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tempo de processamento na GPU igual a 68.1403 msec (aproximadamente 0.01108 segundos)\n",
            "Tempo de processamento na CPU igual a 1.89718 sec\n",
            "Successo! As duas matrizes são iguais, sendo executadas na CPU e na GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tfdW4rFbJToX",
        "colab_type": "code",
        "outputId": "7acf25c9-3489-4e16-fdd6-3eec742ed6b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!ls sample_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anscombe.json\t\t      mnist_test.csv\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sc2tbSE5AwLB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tratamento de Erro\n",
        "\n",
        "Se você alterar consideravelmente o número de blocos e threads por bloco nos exemplos acima, você pode notar alguns casos em que você não receba a resposta esperada. Até este ponto, não adicionamos nenhum tipo de verificação de erros, o que torna muito difícil dizer por que um problema está ocorrendo. A verificação de erros é tão importante quando a programação para um GPU quanto para uma CPU. Então vamos adicionar uma verificação de erro e ver se podemos introduzir alguns erros para capturar.\n",
        "\n",
        "**Nota**: É altamente encorajado que você inclua verificação de erros em seu código sempre que possível!"
      ]
    },
    {
      "metadata": {
        "id": "haDx2vsFAwLC",
        "colab_type": "code",
        "outputId": "c444ebc1-9851-4a91-9831-dee69a981cda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "cell_type": "code",
      "source": [
        "!cat exemplo4.cu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "// Número de elementos em cada vetor\n",
            "#define N 2048 * 2048\n",
            "\n",
            "__global__ void my_kernel(float scalar, float * x, float * y)\n",
            "{\n",
            "    // Determina a identificação de thread global exclusiva, por isso sabemos qual elemento processar\n",
            "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
            "    \n",
            "    // Certifique-se de que ainda temos threads disponíveis!\n",
            "    if ( tid < N ) \n",
            "        y[tid] = scalar * x[tid] + y[tid];\n",
            "}\n",
            "\n",
            "int main()\n",
            "{\n",
            "    float *x, *y;\n",
            "\n",
            "    // O número total de bytes por vetor\n",
            "    int size = N * sizeof (float); \n",
            "\n",
            "    cudaError_t ierrAsync;\n",
            "    cudaError_t ierrSync;\n",
            "\n",
            "    // Aloca memória\n",
            "    cudaMallocManaged(&x, size);\n",
            "    cudaMallocManaged(&y, size);\n",
            "\n",
            "    // Inicializa a memória\n",
            "    for( int i = 0; i < N; ++i )\n",
            "    {\n",
            "        x[i] = 1.0f;\n",
            "        y[i] = 2.0f;\n",
            "    }\n",
            "\n",
            "    int threads_per_block = 256;\n",
            "    int number_of_blocks = (N / threads_per_block) + 1;\n",
            "\n",
            "    my_kernel <<< number_of_blocks, threads_per_block >>> ( 2.0f, x, y );\n",
            "\n",
            "    ierrSync = cudaGetLastError();\n",
            "\n",
            "    // Aguarde até que a GPU termine\n",
            "    ierrAsync = cudaDeviceSynchronize(); \n",
            "\n",
            "    // Verifica status de execução\n",
            "    if (ierrSync != cudaSuccess) { printf(\"Sync error: %s\\n\", cudaGetErrorString(ierrSync)); }\n",
            "    if (ierrAsync != cudaSuccess) { printf(\"Async error: %s\\n\", cudaGetErrorString(ierrAsync)); }\n",
            "\n",
            "    // Imprime o erro máximo\n",
            "    float maxError = 0;\n",
            "    for( int i = 0; i < N; ++i )\n",
            "        if (abs(4-y[i]) > maxError) { maxError = abs(4-y[i]); }\n",
            "    printf(\"Max Error: %.5f\", maxError);\n",
            "\n",
            "    // Libera a memória alocada\n",
            "    cudaFree( x ); cudaFree( y );\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DnmnvQIyAwLG",
        "colab_type": "code",
        "outputId": "7c287808-7d39-42de-ded6-efc30713639e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Compila o exemplo4 e executa o programa gerado\n",
        "!nvcc  -o exemplo4_out exemplo4.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Error: 0.00000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xFbOrPE-AwLJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Consultado os Parâmetros da GPU\n",
        "\n",
        "A API de gerenciamento de dispositivos CUDA C / C ++ permite que um programador consulte o número de dispositivos disponíveis em um sistema e os recursos de cada dispositivo. O código simples abaixo ilustra o uso da API de gerenciamento de dispositivos. Depois que o número de dispositivos habilitados para CUDA conectados ao sistema é determinado via `cudaGetDeviceCount()`, um loop sobre esses dispositivos é realizado (observe que os dispositivos são enumerados a partir de 0) e a função `cudaGetDeviceProperties()` é usada para retornar informações sobre um dispositivo em uma variável de tipo `cudaDeviceProp`. "
      ]
    },
    {
      "metadata": {
        "id": "LeVIggyWAwLK",
        "colab_type": "code",
        "outputId": "1e9d90b4-c676-40a9-98df-44108bb6d619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1343
        }
      },
      "cell_type": "code",
      "source": [
        "!cat exemplo5.cu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#include <stdio.h>\n",
            "\n",
            "#define NX 200\n",
            "#define NY 100\n",
            "\n",
            "__global__ void my_kernel2D(float scalar, float * x, float * y)\n",
            "{\n",
            "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
            "    int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
            "    \n",
            "    // Verifica se ainda temos threads antes de executar a operação\n",
            "    if ( row < NX && col < NY ) \n",
            "        y[row * NY + col] = scalar * x[row * NY + col] + y[row * NY + col];\n",
            "}\n",
            "\n",
            "int main()\n",
            "{\n",
            "    float *x, *y;\n",
            "    float maxError = 0;\n",
            "\n",
            "    // Total de bytes por vetor\n",
            "    int size = NX * NY * sizeof (float); \n",
            "\n",
            "    cudaError_t ierrAsync;\n",
            "    cudaError_t ierrSync;\n",
            "\n",
            "    cudaDeviceProp prop;\n",
            "\n",
            "    // Aloca memória\n",
            "    cudaMallocManaged(&x, size);\n",
            "    cudaMallocManaged(&y, size);\n",
            "\n",
            "    // Inicializa memória\n",
            "    for( int i = 0; i < NX*NY; ++i )\n",
            "    {\n",
            "        x[i] = 1.0f;\n",
            "        y[i] = 2.0f;\n",
            "    }\n",
            "\n",
            "    dim3 threads_per_block (32,16,1);\n",
            "    dim3 number_of_blocks ((NX/threads_per_block.x)+1, (NY/threads_per_block.y)+1, 1);\n",
            "\n",
            "    cudaGetDeviceProperties(&prop, 0);\n",
            "    if (threads_per_block.x * threads_per_block.y * threads_per_block.z > prop.maxThreadsPerBlock) {\n",
            "        printf(\"Muitas threads por bloco ... finalizando\\n\");\n",
            "        goto cleanup;\n",
            "    }\n",
            "    if (threads_per_block.x > prop.maxThreadsDim[0]) {\n",
            "        printf(\"Muitas threads na dimensão x ... finalizando\\n\");\n",
            "        goto cleanup;\n",
            "    }\n",
            "    if (threads_per_block.y > prop.maxThreadsDim[1]) {\n",
            "        printf(\"Muitas threads na dimensão y ... finalizando\\n\");\n",
            "        goto cleanup;\n",
            "    }\n",
            "    if (threads_per_block.z > prop.maxThreadsDim[2]) {\n",
            "        printf(\"Muitas threads na dimensão z ... finalizando\\n\");\n",
            "        goto cleanup;\n",
            "    }\n",
            "\n",
            "    my_kernel2D <<< number_of_blocks, threads_per_block >>> ( 2.0f, x, y );\n",
            "\n",
            "    ierrSync = cudaGetLastError();\n",
            "\n",
            "    // Espera a GPU finalizar\n",
            "    ierrAsync = cudaDeviceSynchronize(); \n",
            "    if (ierrSync != cudaSuccess) { printf(\"Sync error: %s\\n\", cudaGetErrorString(ierrSync)); }\n",
            "    if (ierrAsync != cudaSuccess) { printf(\"Async error: %s\\n\", cudaGetErrorString(ierrAsync)); }\n",
            "\n",
            "    // Imprime o erro\n",
            "    for( int i = 0; i < NX*NY; ++i )\n",
            "        if (abs(4-y[i]) > maxError) { maxError = abs(4-y[i]); }\n",
            "    printf(\"Max Error: %.5f\", maxError);\n",
            "\n",
            "cleanup:\n",
            "    // Libera memória alocada\n",
            "    cudaFree( x ); cudaFree( y );\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "of2unynTAwLN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tente digitar tamanhos diferentes na linha de dimensão do bloco, `dim3 threads_per_block (32,16,1);` e verifique se o seu novo controle de propriedade do dispositivo GPU funciona corretamente!\n",
        "\n",
        "À medida que você começa a escrever código de GPU que possivelmente poderia executar em múltiplos ou diferentes tipos de GPUs, você deve usar a capacidade de consultar facilmente cada dispositivo para determinar a configuração ideal para seu código."
      ]
    },
    {
      "metadata": {
        "id": "uwAdlFuvAwLP",
        "colab_type": "code",
        "outputId": "d95e0e71-1fd2-4a15-b305-25cc126f3106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Compila o exemplo5 e executa o programa gerado\n",
        "!nvcc  -o exemplo5_out exemplo5.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Error: 0.00000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "btkype-tAwLV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gerenciamento de Memória\n",
        "\n",
        "É importante perceber que a GPU tem sua própria memória física; Assim como a CPU usa a RAM do sistema para sua memória. Ao executar o código na GPU, temos de garantir que todos os dados necessários sejam copiados primeiro no barramento PCI-Express para a memória da GPU antes de iniciar nossos kernels.\n",
        "\n",
        "* `cudaMalloc ( void** devPtr, size_t size )` - Esta chamada de API é usada para alocar memória na GPU, e é muito semelhante ao uso de `malloc` na CPU. Você fornece o endereço de um ponteiro que apontará para a memória após a conclusão da chamada, assim como o número de bytes a serem alocados.\n",
        "\n",
        "* `cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )` - Também é muito semelhante ao padrão `memcpy`, esta chamada API é usada para copiar dados entre a CPU e o GPU. É preciso um ponteiro de destino, um ponteiro de origem, o número de bytes a copiar e o quarto parâmetro indica qual direção os dados estão viajando: GPU-> CPU, CPU-> GPU ou GPU-> GPU.\n",
        "\n",
        "* `cudaFree ( void* devPtr )` - Usamos essa chamada de API para liberar qualquer memória que alocamos no GPU.\n",
        "\n",
        "* `cudaMallocManaged ( T** devPtr, size_t size );` - aloca `size` bytes na memória gerenciada e armazena em devPtr.\n",
        "\n",
        "* `cudaFree ( void* devPtr )` - Usamos essa chamada de API para liberar qualquer memória que alocamos na memória gerenciada.\n",
        "\n",
        "Depois de ter usado `cudaMallocManaged` para alocar alguns dados, você apenas usa o ponteiro em seu código, independentemente de ser a CPU ou a GPU acessando os dados. Antes da Memória Unificada, normalmente você tinha dois indicadores associados aos dados; Um para a memória da CPU e um para a memória do GPU (geralmente usando o nome da GPU precedido com um `d_` para indicar a memória do dispositivo).\n",
        "\n",
        "A memória gerenciada é sincronizada entre os espaços de memória no lançamento do kernel e quaisquer pontos de sincronização do dispositivo. Isso significa que, nas arquiteturas Kepler e Maxwell, um ponto de sincronização explícito (normalmente `cudaDeviceSynchronize ()`) precisa ser inserido após um lançamento do kernel, mas antes que o host use dados gerados por esse kernel. Visite a documentação CUDA [page on Unified Memory](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd) para mais detalhes sobre memória unificada."
      ]
    },
    {
      "metadata": {
        "id": "w7Qfu6U6AwLW",
        "colab_type": "code",
        "outputId": "991970f0-4fce-44ff-9dd2-6988840eb9d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "cell_type": "code",
      "source": [
        "!cat exemplo6.cu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#include <string.h>\n",
            "#include <stdio.h>\n",
            "\n",
            "struct DataElement\n",
            "{\n",
            "  char *name;\n",
            "  int value;\n",
            "};\n",
            "\n",
            "__global__ void Kernel(DataElement *elem) {\n",
            "  printf(\"On device: name=%s, value=%d\\n\", elem->name, elem->value);\n",
            "\n",
            "  elem -> name[0] = 'd';\n",
            "  elem -> value++;\n",
            "}\n",
            "\n",
            "void launch(DataElement *elem) {\n",
            "  Kernel <<< 1, 1 >>> (elem);\n",
            "  cudaDeviceSynchronize();\n",
            "}\n",
            "\n",
            "int main(void)\n",
            "{\n",
            "  DataElement *e;\n",
            "  cudaMallocManaged((void**)&e, sizeof(DataElement));\n",
            "\n",
            "  e->value = 10;\n",
            "  cudaMallocManaged((void**)&(e->name), sizeof(char) * (strlen(\"hello\") + 1) );\n",
            "  strcpy(e->name, \"hello\");\n",
            "\n",
            "  launch(e);\n",
            "\n",
            "  printf(\"On host: name=%s, value=%d\\n\", e->name, e->value);\n",
            "\n",
            "  cudaFree(e->name);\n",
            "  cudaFree(e);\n",
            "\n",
            "  cudaDeviceReset();\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IydSUh1zAwLb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Você pode ver por que a memória unificada é atraente - ela remove o requisito de código de gerenciamento de dados complexo. Permitindo que você obtenha suas funções executando na GPU com menos esforço de desenvolvimento."
      ]
    },
    {
      "metadata": {
        "id": "Q38PXLS4AwLd",
        "colab_type": "code",
        "outputId": "3d7d5f3a-07be-411f-f0ff-e73708715f68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Compila o exemplo6 e executa o programa gerado\n",
        "!nvcc -o exemplo6_out exemplo6.cu -run"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On device: name=hello, value=10\n",
            "On host: name=dello, value=11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QMhPSKHtAwLk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transposta da Matriz"
      ]
    },
    {
      "metadata": {
        "id": "efRY8dvaAwLl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neste exemplo vamos programar um algoritmo para [Transposta da Matriz](http://en.wikipedia.org/wiki/Transpose).  Por motivos de simplicidade, usaremos matrizes quadradas. Isso nos permitirá focar as importantes técnicas de otimização de memória sem se preocupar com matrizes de forma desigual. \n",
        "\n",
        "O algoritmo de transposição da matriz é definido como $A_{i,j} = B_{j,i}$ onde $A$ e $B$ são $M \\times M$ matrizes e os índices $i,j$ são os índices de linha e coluna, respectivamente.  (Nos exercícios de hoje vamos usar [column-major](http://en.wikipedia.org/wiki/Row-major_order#Column-major_order) para ordenação dos elementos.)\n",
        "\n",
        "Por exemplo, se você tem um $3 \\times 3$ matriz $A$ como a seguinte $$A = \\left( \\begin{array}{ccc}\n",
        "a & d & g \\\\\n",
        "b & e & h \\\\\n",
        "c & f & i \\end{array} \\right),$$\n",
        "então a transposta da matriz dado por $A^{T}$ é\n",
        "$$A^{T} = \\left( \\begin{array}{ccc}\n",
        "a & b & c \\\\\n",
        "d & e & f \\\\\n",
        "g & h & i \\end{array} \\right).$$\n",
        "\n",
        "Este exemplo consiste em três tarefas. "
      ]
    },
    {
      "metadata": {
        "id": "Hr7WzK3CAwLn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Error Checking\n",
        "\n",
        "Uma das técnicas de programação mais importantes para escrever código robusto é fazer uma verificação de erros adequada. Todas as funções de tempo de execução em CUDA retornam um código de erro do tipo ** `cudaError_t` **. É uma boa prática verificar o código de erro retornado de todas as funções CUDA. Neste exemplo 7, fornecemos duas macros para ajudá-lo a fazer isso. Primeiro, você pode usar `CUDA_CALL (F)` para envolver cada chamada que você faz na API de tempo de execução do CUDA. Por exemplo, em vez de escrever\n",
        "\n",
        "```cpp\n",
        "cudaMemcpy( h_c, c, sizeof(float), cudaMemcpyHostToDevice );\n",
        "```\n",
        "\n",
        "você poderia escrever\n",
        "\n",
        "```cpp\n",
        "CUDA_CALL( cudaMemcpy( h_c, c, sizeof(float), cudaMemcpyHostToDevice ) );\n",
        "```\n",
        "\n",
        "e isso irá verificar o código de retorno do `cudaMemcpy` e informá-lo se houver um erro.\n",
        "\n",
        "Existe uma exceção para esse uso e é quando se chama kernels. Kernels não retornam nenhum valor. Para verificar se um kernel foi iniciado corretamente, você pode fazer o seguinte. Se você tiver um lançamento do kernel\n",
        "\n",
        "```cpp\n",
        "kernel<<< 256, 256 >>>( d_a, d_b, d_c );\n",
        "```\n",
        "\n",
        "você usaria a macro `CUDA_CHECK()` seguida por `CUDA_CALL( cudaDeviceSynchronize )` conforme abaixo\n",
        "\n",
        "```cpp\n",
        "kernel<<< 256, 256 >>>( d_a, d_b, d_c );\n",
        "CUDA_CHECK()\n",
        "CUDA_CALL( cudaDeviceSynchronize() );`\n",
        "```\n",
        "\n",
        "Nas macros de verificação de erros que fornecemos, se houver um erro, você receberá uma mensagem impressa na tela e o programa terminará. Se nenhum erro for detectado, o programa executará normalmente."
      ]
    },
    {
      "metadata": {
        "id": "9asnowATAwLo",
        "colab_type": "code",
        "outputId": "eb1e648e-65af-49db-e877-ad0fc397b722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3723
        }
      },
      "cell_type": "code",
      "source": [
        "!cat exemplo7.cu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/*\n",
            " *  Copyright 2014 NVIDIA Corporation\n",
            " *\n",
            " *  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            " *  you may not use this file except in compliance with the License.\n",
            " *  You may obtain a copy of the License at\n",
            " *\n",
            " *      http://www.apache.org/licenses/LICENSE-2.0\n",
            " *\n",
            " *  Unless required by applicable law or agreed to in writing, software\n",
            " *  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            " *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            " *  See the License for the specific language governing permissions and\n",
            " *  limitations under the License.\n",
            " */\n",
            "\n",
            "#include <stdio.h>\n",
            "\n",
            "#ifdef DEBUG\n",
            "#define CUDA_CALL(F)  if( (F) != cudaSuccess ) \\\n",
            "  {printf(\"Error %s at %s:%d\\n\", cudaGetErrorString(cudaGetLastError()), \\\n",
            "   __FILE__,__LINE__); exit(-1);} \n",
            "#define CUDA_CHECK()  if( (cudaPeekAtLastError()) != cudaSuccess ) \\\n",
            "  {printf(\"Error %s at %s:%d\\n\", cudaGetErrorString(cudaGetLastError()), \\\n",
            "   __FILE__,__LINE__-1); exit(-1);} \n",
            "#else\n",
            "#define CUDA_CALL(F) (F)\n",
            "#define CUDA_CHECK() \n",
            "#endif\n",
            "\n",
            "/* definitions of threadblock size in X and Y directions */\n",
            "\n",
            "#define THREADS_PER_BLOCK_X 32\n",
            "#define THREADS_PER_BLOCK_Y 32\n",
            "\n",
            "/* definition of matrix linear dimension */\n",
            "\n",
            "#define SIZE 4096\n",
            "\n",
            "/* macro to index a 1D memory array with 2D indices in column-major order */\n",
            "\n",
            "#define INDX( row, col, ld ) ( ( (col) * (ld) ) + (row) )\n",
            "\n",
            "/* CUDA kernel for naive matrix transpose */\n",
            "\n",
            "__global__ void naive_cuda_transpose( const int m, \n",
            "                                      const double * const a, \n",
            "                                      double * const c )\n",
            "{\n",
            "  const int myRow = blockDim.x * blockIdx.x + threadIdx.x;\n",
            "  const int myCol = blockDim.y * blockIdx.y + threadIdx.y;\n",
            "\n",
            "  if( myRow < m && myCol < m )\n",
            "  {\n",
            "    c[INDX( myRow, myCol, m )] = a[INDX( myCol, myRow, m )];\n",
            "  } /* end if */\n",
            "  return;\n",
            "\n",
            "} /* end naive_cuda_transpose */\n",
            "\n",
            "void host_transpose( const int m, const double * const a, double *c )\n",
            "{\n",
            "\t\n",
            "/* \n",
            " *  naive matrix transpose goes here.\n",
            " */\n",
            " \n",
            "  for( int j = 0; j < m; j++ )\n",
            "  {\n",
            "    for( int i = 0; i < m; i++ )\n",
            "      {\n",
            "        c[INDX(i,j,m)] = a[INDX(j,i,m)];\n",
            "      } /* end for i */\n",
            "  } /* end for j */\n",
            "\n",
            "} /* end host_dgemm */\n",
            "\n",
            "int main( int argc, char *argv[] )\n",
            "{\n",
            "\n",
            "  int size = SIZE;\n",
            "\n",
            "  fprintf(stdout, \"Matrix size is %d\\n\",size);\n",
            "\n",
            "/* declaring pointers for array */\n",
            "\n",
            "  double *h_a, *h_c;\n",
            "  double *d_a, *d_c;\n",
            " \n",
            "  size_t numbytes = (size_t) size * (size_t) size * sizeof( double );\n",
            "\n",
            "/* allocating host memory */\n",
            "\n",
            "  h_a = (double *) malloc( numbytes );\n",
            "  if( h_a == NULL )\n",
            "  {\n",
            "    fprintf(stderr,\"Error in host malloc h_a\\n\");\n",
            "    return 911;\n",
            "  }\n",
            "\n",
            "  h_c = (double *) malloc( numbytes );\n",
            "  if( h_c == NULL )\n",
            "  {\n",
            "    fprintf(stderr,\"Error in host malloc h_c\\n\");\n",
            "    return 911;\n",
            "  }\n",
            "\n",
            "/* allocating device memory */\n",
            "\n",
            "  CUDA_CALL( cudaMalloc( (void**) &d_a, numbytes ) );\n",
            "  CUDA_CALL( cudaMalloc( (void**) &d_c, numbytes ) );\n",
            "\n",
            "/* set result matrices to zero */\n",
            "\n",
            "  memset( h_c, 0, numbytes );\n",
            "  CUDA_CALL( cudaMemset( d_c, 0, numbytes ) );\n",
            "\n",
            "  fprintf( stdout, \"Total memory required per matrix is %lf MB\\n\", \n",
            "     (double) numbytes / 1000000.0 );\n",
            "\n",
            "/* initialize input matrix with random value */\n",
            "\n",
            "  for( int i = 0; i < size * size; i++ )\n",
            "  {\n",
            "    h_a[i] = double( rand() ) / ( double(RAND_MAX) + 1.0 );\n",
            "  }\n",
            "\n",
            "/* copy input matrix from host to device */\n",
            "\n",
            "  CUDA_CALL( cudaMemcpy( d_a, h_a, numbytes, cudaMemcpyHostToDevice ) );\n",
            "\n",
            "/* create and start timer */\n",
            "\n",
            "  cudaEvent_t start, stop;\n",
            "  CUDA_CALL( cudaEventCreate( &start ) );\n",
            "  CUDA_CALL( cudaEventCreate( &stop ) );\n",
            "  CUDA_CALL( cudaEventRecord( start, 0 ) );\n",
            "\n",
            "/* call naive cpu transpose function */\n",
            "\n",
            "  host_transpose( size, h_a, h_c );\n",
            "\n",
            "/* stop CPU timer */\n",
            "\n",
            "  CUDA_CALL( cudaEventRecord( stop, 0 ) );\n",
            "  CUDA_CALL( cudaEventSynchronize( stop ) );\n",
            "  float elapsedTime;\n",
            "  CUDA_CALL( cudaEventElapsedTime( &elapsedTime, start, stop ) );\n",
            "\n",
            "/* print CPU timing information */\n",
            "\n",
            "  fprintf(stdout, \"Total time CPU is %f sec\\n\", elapsedTime / 1000.0f );\n",
            "  fprintf(stdout, \"Performance is %f GB/s\\n\", \n",
            "    8.0 * 2.0 * (double) size * (double) size / \n",
            "    ( (double) elapsedTime / 1000.0 ) * 1.e-9 );\n",
            "\n",
            "/* setup threadblock size and grid sizes */\n",
            "\n",
            "  dim3 threads( THREADS_PER_BLOCK_X, THREADS_PER_BLOCK_Y, 1 );\n",
            "  dim3 blocks( ( size / THREADS_PER_BLOCK_X ) + 1, \n",
            "               ( size / THREADS_PER_BLOCK_Y ) + 1, 1 );\n",
            "\n",
            "/* start timers */\n",
            "  CUDA_CALL( cudaEventRecord( start, 0 ) );\n",
            "\n",
            "/* call naive GPU transpose kernel */\n",
            "\n",
            "  naive_cuda_transpose<<< blocks, threads >>>( size, d_a, d_c );\n",
            "  CUDA_CHECK()\n",
            "  CUDA_CALL( cudaDeviceSynchronize() );\n",
            "\n",
            "/* stop the timers */\n",
            "\n",
            "  CUDA_CALL( cudaEventRecord( stop, 0 ) );\n",
            "  CUDA_CALL( cudaEventSynchronize( stop ) );\n",
            "  CUDA_CALL( cudaEventElapsedTime( &elapsedTime, start, stop ) );\n",
            "\n",
            "/* print GPU timing information */\n",
            "\n",
            "  fprintf(stdout, \"Total time GPU is %f sec\\n\", elapsedTime / 1000.0f );\n",
            "  fprintf(stdout, \"Performance is %f GB/s\\n\", \n",
            "    8.0 * 2.0 * (double) size * (double) size / \n",
            "    ( (double) elapsedTime / 1000.0 ) * 1.e-9 );\n",
            "\n",
            "/* copy data from device to host */\n",
            "\n",
            "  CUDA_CALL( cudaMemset( d_a, 0, numbytes ) );\n",
            "  CUDA_CALL( cudaMemcpy( h_a, d_c, numbytes, cudaMemcpyDeviceToHost ) );\n",
            "\n",
            "/* compare GPU to CPU for correctness */\n",
            "\n",
            "  for( int j = 0; j < size; j++ )\n",
            "  {\n",
            "    for( int i = 0; i < size; i++ )\n",
            "    {\n",
            "      if( h_c[INDX(i,j,size)] != h_a[INDX(i,j,size)] ) \n",
            "      {\n",
            "        printf(\"Error in element %d,%d\\n\", i,j );\n",
            "        printf(\"Host %f, device %f\\n\",h_c[INDX(i,j,size)],\n",
            "                                      h_a[INDX(i,j,size)]);\n",
            "        printf(\"FAIL\\n\");\n",
            "        goto end;\n",
            "      } /* end fi */\n",
            "    } /* end for i */\n",
            "  } /* end for j */\n",
            "\n",
            "/* free the memory */\n",
            "  printf(\"PASS\\n\");\n",
            "\n",
            "  end:\n",
            "  free( h_a );\n",
            "  free( h_c );\n",
            "  CUDA_CALL( cudaFree( d_a ) );\n",
            "  CUDA_CALL( cudaFree( d_c ) );\n",
            "  CUDA_CALL( cudaDeviceReset() );\n",
            "\n",
            "  return 0;\n",
            "} /* end main */\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5jmog9zCAwLs",
        "colab_type": "code",
        "outputId": "edcbded0-02da-467d-a81f-b3c628b17556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Compila o exemplo7\n",
        "!nvcc -lineinfo -DDEBUG -arch=sm_30 -o exemplo7_out exemplo7.cu && echo Compilado com Sucesso!"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compilado com Sucesso!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NDBFJi93AwLv",
        "colab_type": "code",
        "outputId": "156fc5e1-c365-4de4-f838-e246f5c58a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "# Executa o exemplo7\n",
        "!./exemplo7_out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix size is 4096\n",
            "Total memory required per matrix is 134.217728 MB\n",
            "Total time CPU is 0.372804 sec\n",
            "Performance is 0.720043 GB/s\n",
            "Total time GPU is 0.005972 sec\n",
            "Performance is 44.952137 GB/s\n",
            "PASS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iV4tBR2IAwL0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Se quiser gerar um zip com todos os arquivos criados, execute a célula abaixo."
      ]
    },
    {
      "metadata": {
        "id": "2fe3gwncAwL1",
        "colab_type": "code",
        "outputId": "895b9a72-f6f4-4e94-9e26-5ebdf33ac9ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -f cuda_files.zip\n",
        "zip -r cuda_files.zip . -i exemplo*.*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: exemplo5.cu (deflated 63%)\n",
            "  adding: exemplo4.cu (deflated 53%)\n",
            "  adding: exemplo1.cu (deflated 31%)\n",
            "  adding: exemplo3.cu (deflated 64%)\n",
            "  adding: exemplo7.cu (deflated 65%)\n",
            "  adding: exemplo2.cu (deflated 52%)\n",
            "  adding: exemplo6.cu (deflated 51%)\n",
            "  adding: exemplo7 (1).cu (deflated 65%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dkm0WZogUttd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download a file"
      ]
    },
    {
      "metadata": {
        "id": "x4ds4AfvUnLf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cuda_files.zip') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q5l1qxflTsJB",
        "colab_type": "code",
        "outputId": "1183a0a5-4fc2-441d-d76c-605a360aefc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " cuda_files.zip   exemplo3.cu\t exemplo4_out   exemplo6.cu\t   exemplo7.cu\n",
            " exemplo1.cu\t  exemplo3_out\t exemplo5.cu    exemplo6_out\t   exemplo7_out\n",
            " exemplo2.cu\t  exemplo4.cu\t exemplo5_out  'exemplo7 (1).cu'   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0aNp6dOyAwL5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Depois de ** executar a célula acima, você pode baixar o arquivo zip [here](cuda_files.zip)"
      ]
    }
  ]
}